{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 - Model Evaluation",
        "",
        "Analyze prediction results and identify areas for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd",
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "from pathlib import Path",
        "",
        "BASE_PATH = Path('..')",
        "MODEL_PATH = BASE_PATH / 'model_evaluation'",
        "FEATURES_PATH = BASE_PATH / 'features_v2'",
        "",
        "# Load predictions",
        "pred = pd.read_csv(MODEL_PATH / 'predictions_h2.csv')",
        "print(f\"Loaded predictions: {len(pred):,} rows\")",
        "pred.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overall Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wmape(actual, predicted):",
        "    total = np.sum(np.abs(actual))",
        "    if total == 0:",
        "        return 999",
        "    return 100 * np.sum(np.abs(actual - predicted)) / total",
        "",
        "overall_wmape = wmape(pred['actual'].values, pred['predicted'].values)",
        "overall_mae = np.abs(pred['actual'] - pred['predicted']).mean()",
        "",
        "print(\"=== Overall Metrics ===\")",
        "print(f\"WMAPE: {overall_wmape:.1f}%\")",
        "print(f\"MAE: {overall_mae:,.0f}\")",
        "print(f\"Total actual: {pred['actual'].sum():,.0f}\")",
        "print(f\"Total predicted: {pred['predicted'].sum():,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics by SKU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate per-SKU metrics",
        "sku_metrics = pred.groupby('sku').apply(",
        "    lambda g: pd.Series({",
        "        'actual_total': g['actual'].sum(),",
        "        'predicted_total': g['predicted'].sum(),",
        "        'mae': np.abs(g['actual'] - g['predicted']).mean(),",
        "        'wmape': wmape(g['actual'].values, g['predicted'].values),",
        "        'n_weeks': len(g)",
        "    })",
        ").reset_index()",
        "",
        "print(f\"\\n=== SKU Metrics Summary ===\")",
        "print(f\"Total SKUs: {len(sku_metrics)}\")",
        "print(f\"\\nWMAPE distribution:\")",
        "print(sku_metrics['wmape'].describe())",
        "",
        "# Classify by accuracy",
        "sku_metrics['accuracy_tier'] = pd.cut(",
        "    sku_metrics['wmape'], ",
        "    bins=[0, 20, 40, 60, 100, 1000],",
        "    labels=['Excellent (<20%)', 'Good (20-40%)', 'Fair (40-60%)', 'Poor (60-100%)', 'Very Poor (>100%)']",
        ")",
        "",
        "print(f\"\\nAccuracy Distribution:\")",
        "print(sku_metrics['accuracy_tier'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best & Worst Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top 10 best predictions (lowest WMAPE)",
        "print(\"=== Top 10 Best Predictions ===\")",
        "best = sku_metrics.nsmallest(10, 'wmape')[['sku', 'wmape', 'actual_total', 'n_weeks']]",
        "print(best.to_string(index=False))",
        "",
        "# Top 10 worst predictions",
        "print(\"\\n=== Top 10 Worst Predictions ===\")",
        "worst = sku_metrics.nlargest(10, 'wmape')[['sku', 'wmape', 'actual_total', 'n_weeks']]",
        "print(worst.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Sample Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick 4 SKUs to visualize (mix of good and bad)",
        "sample_skus = list(sku_metrics.nsmallest(2, 'wmape')['sku']) + list(sku_metrics.nlargest(2, 'wmape')['sku'])",
        "",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))",
        "axes = axes.flatten()",
        "",
        "for ax, sku in zip(axes, sample_skus):",
        "    sku_data = pred[pred['sku'] == sku].sort_values('year_week')",
        "    sku_wmape = sku_metrics[sku_metrics['sku'] == sku]['wmape'].values[0]",
        "    ",
        "    ax.plot(range(len(sku_data)), sku_data['actual'], 'b-o', label='Actual')",
        "    ax.plot(range(len(sku_data)), sku_data['predicted'], 'r--x', label='Predicted')",
        "    ax.set_title(f'SKU: {sku} (WMAPE: {sku_wmape:.1f}%)')",
        "    ax.set_xlabel('Week')",
        "    ax.legend()",
        "",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identify Problem Areas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SKUs with high error but high volume (important to fix)",
        "high_volume = sku_metrics[sku_metrics['actual_total'] > sku_metrics['actual_total'].median()]",
        "problem_skus = high_volume[high_volume['wmape'] > 50].sort_values('actual_total', ascending=False)",
        "",
        "print(f\"=== High-Volume SKUs with Poor Predictions ===\")",
        "print(f\"Found {len(problem_skus)} SKUs with >50% WMAPE and above-median volume\")",
        "print(problem_skus[['sku', 'wmape', 'actual_total', 'n_weeks']].head(15).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save SKU metrics",
        "sku_metrics.to_csv(MODEL_PATH / 'sku_evaluation_metrics.csv', index=False)",
        "print(f\"\u2713 Saved sku_evaluation_metrics.csv\")",
        "",
        "# Summary stats",
        "summary = {",
        "    'total_skus': len(sku_metrics),",
        "    'overall_wmape': overall_wmape,",
        "    'skus_under_20_wmape': (sku_metrics['wmape'] < 20).sum(),",
        "    'skus_under_40_wmape': (sku_metrics['wmape'] < 40).sum(),",
        "    'skus_over_100_wmape': (sku_metrics['wmape'] > 100).sum()",
        "}",
        "",
        "print(f\"\\n=== Summary ===\")",
        "for k, v in summary.items():",
        "    print(f\"{k}: {v}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}